
Abstract
OpenEssayist is an automated, interactive feedback system designed to provide an acceptable level of support for
students as they write essays for summative assessment. There are two main components to the system: (1) a
linguistic analysis engine and (2) a web application that generates feedback for students The main pedagogical
challenge in the e-assessment of free text is how to provide meaningful “advice for action” in order to support
students writing their summative assessments. We have built a first working version of the system in which we use
unsupervised graph-based ranking algorithms (following Mihalcea & Tarau, 2005) to automatically extract key
words, phrases and sentences from student essays. We have developed several external representations of these
summarisation techniques. For examples, key words and key phrases can be viewed in a word cloud or in a
dispersion graph, and they can be explored and organised into groups. Holistic approaches have also been tested
using ‘mash ups’ where key words and key sentences are highlighted in context in the essay itself, helping students
to investigate the distribution of key words and its potential implications for the clarity of the narrative. This paper
will report the findings from our pilot studies of the interactive models associated with the summarisation
techniques.
Keywords
Automated feedback; essay writing; summary; external representations
Introduction
OpenEssayist is a web application that has been designed to assist students while writing their essays
for summative assessment. 'Summative assessment' is defined as “an assessment that is administered at
the end of a learning sequence. It is designed to form a judgement about learning that is often reported
in terms of grades or scores and is underpinned by a set of quality assurance processes” (Whitelock,
2011, p. 341). Students who are new to writing essays in higher education and who have returned after
a break of many years to undertake a Masters Level qualification often experience difficulty in writing
their first summative essay and ‘drop out’ of the course (Simpson, 2003). Therefore the goal of this
application is to provide students with feedback on their draft essays before they submit them for
summative assessment.
OpenEssayist consists of two major components (1) a linguistic engine and (2) a web application that
generates feedback for students (Van Labeke et al, 2013). Understanding how to provide appropriate
feedback to students to enable them to move forward with their essay writing is the focus of this paper
since providing meaningful feedback or “advice for action” (Whitelock, 2011) needed to be user tested
before the system went live in September 2013. A round of supervised, observed user testing was
therefore organised, having six participants.
This paper reports on how well the participants understood a range of external representations
generated by OpenEssayist in order to assist with essay improvements before submission. The findings
have informed the selection of the final representations that will be used for students following a
postgraduate module entitled “Accessible online learning: Supporting disabled students". This
postgraduate module runs in September 2013 for about 20 weeks and contributes to a Master of Arts
2
(MA) in Online and Distance Education. All modules, materials and support are delivered online.
Students on this module, as is the case for most of the students at the Open University (OU), are parttime,
mature students, many of whom have not been in formal education for a long period of time.
The user testing was designed to understand how to provide meaningful representations of the analysis
of draft essays by exploring text analysis outputs and visual analysis outputs
OpenEssayist: The Linguistic Engine
Our approach is to extract different kinds of summaries from the student user's essay, and to present
them to her in different ways using a variety of external visualisations. The approach is primarily
focused on user understanding and self-directed learning, rather than on essay improvement, and it
engages the user on matters of content, rather than pointing out failings in grammar, style, and
structure. Our approach contrasts strongly with established Automated Essay Scoring and Automed
Writing Evaluation systems (including Criterion (Burstein et al, 2003), Pearson’s WriteToLearn (based
on Landauer’s Intelligent Essay Assessor (Landauer et al, 2003) and Summary Street (Franzke and
Streeter, 2006)), IntelliMetric (Rudner et al, 2006) and LightSIDE (Mayfield and Rosé, 2013)). Rather
than telling the user in detail how to fix any incorrect and poor attributes of her essay, our system
encourages the user to reflect on the content of her essay. It uses linguistic technologies, graphics,
animations, and interactive exercises to enable the user to comprehend the content of her essay more
objectively, and to reflect on whether the essay adequately conveys her intended meanings.
Essay Analysis Output
To run OpenEssayist, the test participant pasted her essay into a web form and selected 'analyse'. The
system first returned a representation of the essay that looked very similar to the input, but in which
some structural components were identified by different colours (introduction, conclusion, discussion,
title, heading, etc.).
In a different view, the key words and key phrases of the essay were presented as a list arranged in rank
order of 'key-ness' (which can be thought of as importance, or as representativeness of the text as a
whole). In other views, key sentences (those most representative of the text as a whole) were presented
both inline in the text 'mash up' view, and in a list in order of importance.
In addition to the core summaries of the essay (key words, key phrases and key sentences), additional
specialised data structures were made available, including a 'chord diagram' depicting the results of the
key sentence analysis, an 'adjacency matrix' depicting the same, visualised networks depicting the key
sentence and key word analyses, and the intersection between the essay's key phrases and words found
in the assignment question.
The system was developed by experimenting with a corpus of 267 student essays. While developing
the system, much effort has also been devoted to observing and measuring a wide variety of essay
attributes (Field et al, 2013). We continue look for ways to exploit these results and, ultimately, to
devise effective models of feedback informed by them.
The Empirical Study
The user testing adopted a cognitive walkthrough (Lewis et al, 1990; Polson et al, 1992) as a usability
inspected method since it focuses on evaluating a design for user learning through exploring the
system, i.e. OpenEssayist. We suspect that our students will prefer to learn how to use the system
without resorting to a user guide as supported by the findings of Carroll & Rosson (1987) and Fischer
(1991). The system is embodied within an incremental approach to learning and so users will learn how
to use and work with features that are of benefit to them. In our case, this means producing external
representations of the analysis that will easily translate into changes that students will make to their
draft essays.
Participants
Six adults from the Institute of Educational Technology volunteered to take part in the empirical study.
They all had experience of either writing summative essays for modules at the Open University UK or
had designed essay questions for Open University modules.


(http://oro.open.ac.uk/41844/)

---
